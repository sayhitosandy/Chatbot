800/800 [==============================] - 16s 21ms/step - loss: 6.0796 - val_loss: 5.5408              Epoch 3/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 4.9617 - val_loss: 4.5426              Epoch 4/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 4.1052 - val_loss: 3.8284              Epoch 5/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 3.4600 - val_loss: 3.3026              Epoch 6/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 3.0221 - val_loss: 2.9819              Epoch 7/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 2.7468 - val_loss: 2.7530              Epoch 8/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 2.5725 - val_loss: 2.6327              Epoch 9/30                                                                                              800/800 [==============================] - 17s 21ms/step - loss: 2.4794 - val_loss: 2.5644              Epoch 10/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.4201 - val_loss: 2.5215              Epoch 11/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.3726 - val_loss: 2.4823              Epoch 12/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.3321 - val_loss: 2.4528              Epoch 13/30                                                                                             800/800 [==============================] - 18s 23ms/step - loss: 2.2973 - val_loss: 2.4298              Epoch 14/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2699 - val_loss: 2.4148              Epoch 15/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2487 - val_loss: 2.4084              Epoch 16/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2329 - val_loss: 2.3991              Epoch 17/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2206 - val_loss: 2.4032              Epoch 18/30                                                                                             800/800 [==============================] - 16s 21ms/step - loss: 2.2110 - val_loss: 2.3945              Epoch 19/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.2024 - val_loss: 2.4067              Epoch 20/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1950 - val_loss: 2.3947              Epoch 21/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1881 - val_loss: 2.3967              Epoch 22/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1810 - val_loss: 2.3992              Epoch 23/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1751 - val_loss: 2.3997              Epoch 24/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1690 - val_loss: 2.4111              Epoch 25/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1632 - val_loss: 2.4012              Epoch 26/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1579 - val_loss: 2.4110              Epoch 27/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1524 - val_loss: 2.4140              Epoch 28/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1473 - val_loss: 2.4178              Epoch 29/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1424 - val_loss: 2.4226              Epoch 30/30                                                                                             800/800 [==============================] - 14s 17ms/step - loss: 2.1377 - val_loss: 2.4286              Saving...                                                                                               C:\Users\Vishal\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\engine\topology.py:2344: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 3) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 3) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).                                                                                                        str(node.arguments) + '. They will not be included '                                                  -                                                                                                       Input sentence:  not the hacking and gagging and spitting part. please.                                 Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  you re asking me out. that s so cute. what s your name again?                          Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  gosh if only we could find kat a boyfriend...                                          Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  c esc ma tete. this is my head                                                         Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  that s because it s such a nice one.                                                   Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  how is our little find the wench a date plan progressing?                              Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  you have my word. as a gentleman                                                       Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  how do you get your hair to look like that?                                            Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  sure have.                                                                             Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>