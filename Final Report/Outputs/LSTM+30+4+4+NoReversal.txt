800/800 [==============================] - 16s 20ms/step - loss: 5.7610 - val_loss: 5.0612              Epoch 3/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 4.3792 - val_loss: 3.9573              Epoch 4/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 3.4869 - val_loss: 3.2946              Epoch 5/30                                                                                              800/800 [==============================] - 16s 19ms/step - loss: 2.9535 - val_loss: 2.9194              Epoch 6/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 2.6537 - val_loss: 2.7185              Epoch 7/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 2.5036 - val_loss: 2.6188              Epoch 8/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 2.4222 - val_loss: 2.5614              Epoch 9/30                                                                                              800/800 [==============================] - 16s 20ms/step - loss: 2.3613 - val_loss: 2.5130              Epoch 10/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.3123 - val_loss: 2.4766              Epoch 11/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.2736 - val_loss: 2.4525              Epoch 12/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.2457 - val_loss: 2.4362              Epoch 13/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.2252 - val_loss: 2.4286              Epoch 14/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.2105 - val_loss: 2.4241              Epoch 15/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1998 - val_loss: 2.4254              Epoch 16/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1912 - val_loss: 2.4316              Epoch 17/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1839 - val_loss: 2.4328              Epoch 18/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.1772 - val_loss: 2.4326              Epoch 19/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1707 - val_loss: 2.4338              Epoch 20/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1643 - val_loss: 2.4399              Epoch 21/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1583 - val_loss: 2.4377              Epoch 22/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1520 - val_loss: 2.4432              Epoch 23/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1461 - val_loss: 2.4475              Epoch 24/30                                                                                             800/800 [==============================] - 16s 20ms/step - loss: 2.1411 - val_loss: 2.4423              Epoch 25/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.1357 - val_loss: 2.4572              Epoch 26/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.1306 - val_loss: 2.4603              Epoch 27/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.1254 - val_loss: 2.4523              Epoch 28/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.1199 - val_loss: 2.4602              Epoch 29/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.1139 - val_loss: 2.4769              Epoch 30/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.1080 - val_loss: 2.4934              Saving...                                                                                               C:\Users\Vishal\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\engine\topology.py:2344: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 4) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).                                                                                                        str(node.arguments) + '. They will not be included '                                                  -                                                                                                       Input sentence:  not the hacking and gagging and spitting part. please.                                 Decoded sentence:  you <UNK> <UNK> <UNK> <UNK> <UNK>                                                    -                                                                                                       Input sentence:  you re asking me out. that s so cute. what s your name again?                          Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  gosh if only we could find kat a boyfriend...                                          Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  c esc ma tete. this is my head                                                         Decoded sentence:  you <UNK> <UNK> <UNK> <UNK> <UNK>                                                    -                                                                                                       Input sentence:  that s because it s such a nice one.                                                   Decoded sentence:  you <UNK> <UNK> <UNK> <UNK> <UNK>                                                    -                                                                                                       Input sentence:  how is our little find the wench a date plan progressing?                              Decoded sentence:  you <UNK> <UNK> <UNK> <UNK> <UNK>                                                    -                                                                                                       Input sentence:  you have my word. as a gentleman                                                       Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  how do you get your hair to look like that?                                            Decoded sentence:  you <UNK> <UNK> <UNK> <UNK> <UNK>                                                    -                                                                                                       Input sentence:  sure have.                                                                             Decoded sentence:  you <UNK> <UNK> <UNK> <UNK> <UNK>