800/800 [==============================] - 16s 20ms/step - loss: 7.2450 - val_loss: 6.8618              Epoch 2/30                                                                                              800/800 [==============================] - 14s 18ms/step - loss: 6.4587 - val_loss: 6.0477              Epoch 3/30                                                                                              800/800 [==============================] - 14s 18ms/step - loss: 5.6022 - val_loss: 5.1994              Epoch 4/30                                                                                              800/800 [==============================] - 15s 18ms/step - loss: 4.8442 - val_loss: 4.5351              Epoch 5/30                                                                                              800/800 [==============================] - 15s 18ms/step - loss: 4.2265 - val_loss: 3.9767              Epoch 6/30                                                                                              800/800 [==============================] - 14s 18ms/step - loss: 3.7035 - val_loss: 3.5082              Epoch 7/30                                                                                              800/800 [==============================] - 14s 18ms/step - loss: 3.2885 - val_loss: 3.1641              Epoch 8/30                                                                                              800/800 [==============================] - 14s 18ms/step - loss: 3.0017 - val_loss: 2.9246              Epoch 9/30                                                                                              800/800 [==============================] - 17s 22ms/step - loss: 2.8067 - val_loss: 2.7961              Epoch 10/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.6720 - val_loss: 2.6388              Epoch 11/30                                                                                             800/800 [==============================] - 18s 22ms/step - loss: 2.5500 - val_loss: 2.5532              Epoch 12/30                                                                                             800/800 [==============================] - 15s 19ms/step - loss: 2.4807 - val_loss: 2.4997              Epoch 13/30                                                                                             800/800 [==============================] - 20s 25ms/step - loss: 2.4304 - val_loss: 2.4585              Epoch 14/30                                                                                             800/800 [==============================] - 19s 24ms/step - loss: 2.3876 - val_loss: 2.4221              Epoch 15/30                                                                                             800/800 [==============================] - 15s 19ms/step - loss: 2.3484 - val_loss: 2.3918              Epoch 16/30                                                                                             800/800 [==============================] - 15s 18ms/step - loss: 2.3151 - val_loss: 2.3693              Epoch 17/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.2900 - val_loss: 2.3537              Epoch 18/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.2715 - val_loss: 2.3444              Epoch 19/30                                                                                             800/800 [==============================] - 15s 18ms/step - loss: 2.2573 - val_loss: 2.3413              Epoch 20/30                                                                                             800/800 [==============================] - 15s 19ms/step - loss: 2.2475 - val_loss: 2.3383              Epoch 21/30                                                                                             800/800 [==============================] - 15s 19ms/step - loss: 2.2396 - val_loss: 2.3367              Epoch 22/30                                                                                             800/800 [==============================] - 15s 19ms/step - loss: 2.2336 - val_loss: 2.3378              Epoch 23/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.2286 - val_loss: 2.3438              Epoch 24/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.2248 - val_loss: 2.3428              Epoch 25/30                                                                                             800/800 [==============================] - 14s 18ms/step - loss: 2.2216 - val_loss: 2.3491              Epoch 26/30                                                                                             800/800 [==============================] - 15s 18ms/step - loss: 2.2191 - val_loss: 2.3528              Epoch 27/30                                                                                             800/800 [==============================] - 18s 22ms/step - loss: 2.2169 - val_loss: 2.3556              Epoch 28/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2153 - val_loss: 2.3614              Epoch 29/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2134 - val_loss: 2.3709              Epoch 30/30                                                                                             800/800 [==============================] - 17s 21ms/step - loss: 2.2116 - val_loss: 2.3728              Saving...                                                                                               C:\Users\Vishal\Anaconda3\envs\tensorflow-gpu\lib\site-packages\keras\engine\topology.py:2344: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 2) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 2) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).                                                                                                        str(node.arguments) + '. They will not be included '                                                  -                                                                                                       Input sentence:  not the hacking and gagging and spitting part. please.                                 Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  you re asking me out. that s so cute. what s your name again?                          Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  gosh if only we could find kat a boyfriend...                                          Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  c esc ma tete. this is my head                                                         Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  that s because it s such a nice one.                                                   Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  how is our little find the wench a date plan progressing?                              Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  you have my word. as a gentleman                                                       Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  how do you get your hair to look like that?                                            Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>                                                  -                                                                                                       Input sentence:  sure have.                                                                             Decoded sentence:  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>